---
layout: post
title:  "Non-stochastic Best Arm Identification and Hyperparameter Optimization"
date:   2021-08-22
categories: post
---

Motivated by the hyperparameter optimisation task, the authors model it as a non-stochastic best-arm idenfication problem. To this end, the authors introduce the non-stochastic setting in the best-arm identification. The paper provides both strong theoretical and empirical proof of efficiency of the Successive Halving algorithm in hyperparameter optimisation.
<!--more-->

{% marginnote 'table-id-1' 'Table 1: Paper summary.' %}

<div class="table-wrapper">
<table class="booktabs" style="margin: 0px auto;">
	<tbody>
		<tr>
			<td>Author(s)</td> <td>Kevin Jamieson, Ameet Talwalkar</td>
		</tr>
		<tr>
			<td>Source</td> <td><a href="https://arxiv.org/abs/1502.07943">https://arxiv.org/abs/1502.07943</a></td>
		</tr>
		<tr>
			<td>Danger level</td> <td>☠️ ☠️ <span style="color:gray">☠ ☠ ☠</span></td>
		</tr>
	</tbody>
</table>
</div>

## Contributions
- Introduction of the non-stochastic setting in the best arm identification problem.
- Analysis of the Successive Halving algorithm in the non-stochastic setting.
- Modelling of the hyperparameter optimisation as a non-stochastic best arm identification problem.
- Empirical proof of efficiency of Successive Halving in hyperparameter optimisation.

## Model parameter and model hyperparameter

Consider this scenario: given two models, $$ \mathcal{M}_{1} $$ and $$ \mathcal{M}_{2} $$, what would differentate them? What makes a model different from another? For the moment, assume that we are dealing with the same kind of models (say, $$ \mathcal{M}_{1} $$ and $$ \mathcal{M}_{2} $$ are both neural networks or both decision trees but not one neural network and the other decision tree).

The answer is their parameters and hyperparameters. For ANN, the parameters are the set of weights; the hyperparameters are the structure of the network, the number of neurons in each layer, the choice of activation function etc. For decision tree, the parameters are the set of split points; the hyperparameters are the criterion, maximum depth, maximum number of features etc.

### Parameter vs. hyperparameter

That said, their is one crucial difference between parameters and hyperparameters. Given the data, the optimal value of the parameters can be *learned* from the data. Hyperparameters, on the other hand, can't be learned *directly*.

### Serial vs. parallel

Each hyperparameter has an associated hyperparameter space $$ \mathcal{H} $$, where $$ \mathcal{H} = \{ h_{1}, h_{2}, \dots, h_{n} \} $${% sidenote 'sn-id-2' "Each hyperparameter choice $$ h \in \mathcal{H} $$ is often called a hyperparameter configuration."%} (for the entirety of this post we assume that $$ \mathcal{H} $$ is finite). For learning rate, for example, the hyperparameter space is $$ \mathbb{R}_{>0} $$ (the set of positive reals). There are at least two different ways in which you can handle the task of finding out the best hyperparameter configuration: *serial* and *parallel*.

Under the *serial* setting we have the following workflow. Choose a suitable hyperparameter configuration from $$ \mathcal{H} $$, run it for an *appropriate* amount of time to get a good estimate of the loss, and then depending on this loss choose a new hyperparameter configuration from $$ \mathcal{H} $$. Continue this as long as time permits.

The *parallel* setting is slightly different. Here we start with *all* the hyperparameter configurations i.e. all $$ h \in \mathcal{H} $$. We allot a certain amount of time for the configurations, get an estimate of the *partial* loss after the alloted time, take some actions if needed, and then allot more time to the configurations. Continue this as long as time permits.

This post is entirely dedicated to this *parallel* setting.

### Hyperparameter optimisation

Hyperparameter optimisation is therefore concerned about finding out the optimal value of the hyperparameter for which the model loss is minimum. Obviously, we have the brute-force approach: starting with a total budget of $$ B $$ units (minutes, hours, clock-cycles etc.) and $$ n $$ hyperparameter choices, allocate $$ \frac{B}{n} $$ units of resource to all the choices. Finally return the hyperparameter with the lowest loss. This is often called the uniform allocation strategy.

But can we do better? Before we answer this question, let's talk a little bit about slot machines and gambling.

## Multi-armed bandit and best arm identification

The problem setting of [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) (MAB) is extremely simple: a gambler enters a room with multiple slot machines and (s)he has a fixed, limited amount of money/chances. (S)he has to play the slot machines in such a way that her/his gain is maximised.

The only problem is that (s)he has no idea which machines are good and which are bad. Good ones yield rewards more easily compared to the bad ones. Thus (s)he has to use trial-and-error to gain knowledge about the machines. Contrary to the simplicity of the problem, good/great solutions are not that straightforward.

MAB has a long history, arguably starting in 1933 with the [classic paper of Thompson](https://www.jstor.org/stable/2332286){% sidenote 'sn-id-2' "Quite strangely, Thompson sampling, as it's known these days, turned out to be quite hard to analyse theoretically (fun fact: the proof that Thompson sampling is asymptotically optimal came out quite recently (2012-13) in a series of papers ([[1]](#1), [[2]](#2), and [[6]](#6)), so that's almost 80 years)."%}. There are many good, quick introductory resources available online--[this video](https://youtu.be/bkw6hWvh_3k) by Robert C. Gray (aka Academic Gamer) is a great starting point. If you want something more in-depth, take a look at [this excelent post](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html) by Lilian Weng.

Let us now shift our attention towards a slightly different, yet related problem--the best-arm identification problem{% sidenote 'sn-id-2' "It's also known as the pure exploration problem in the bandit literature."%}. Here we simply want to identify the *best* arm{% sidenote 'sn-id-2' "We use the terms *arm* and *slot machine* interchangeably."%}. The precise definition of *best*, as we're going to see next, is dependant on the problem setting. 

So after $$ T $$ rounds, our agent is going to be judged based on the quality of the slot machine that it recommends. In practice, though, we deal with a slightly more generalised problem: instead of a single recommendation after $$ T $$ rounds, we want the agent to recommend a slot machine in each round. Check out [[3]](#3) for more details.

## Stochastic vs. non-stochastic setting

We now discuss how define the *best* arm. Since we'll be talking about model *losses* later, let's be consistent and say that each time we pull the lever of a slot machine we observe a *loss* instead of a *reward*. We want to find out the slot with the lowest loss therefore. Say $$ \ell_{i, t} $$ denotes the loss observed on the $$ t $$-th pull of the $$ i $$-th slot machine.

In the stochastic setting each time one pulls a slot machine a reward is sampled from a fixed distribution with mean $$ \mu $${% sidenote 'sn-id-1' "The notation that I'll be using differs slightly from what the authors chose to use. I'll be using $$ \mu $$ (more standard, in my opinion) instead of $$ \nu $$ to denote the mean of the distribution in the stochastic setting. For the non-stochastic setting, I'll be consistent with the authors and use $$ \nu $$ to denote the limiting loss."%} i.e. 

$$ \mathbb{E}[\ell_{i, t}] = \mu_{i} \text{.} $$

In other words, if we take a large number of such losses from the $$ k $$-th slot machine (or rewards, if that makes more sense) and compute the average, it would be exactly or very close to $$ \mu_{k} $$.

The non-stochastic setting, on the other hand, says that there exists a finite *limiting loss* for each slot machine; each time one pulls the slot machine the generated loss would be a little bit closer to that *limiting loss*. In mathematical terms it is written as

$$ \lim_{t \to \infty} \ell_{i, t} = \nu_{k} \text{.} $$

<script src='https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js'></script>
<script src='../../assets/scripts/reward.js'></script>
<script src='../../assets/scripts/utils.js'></script>

<canvas id="canvasStochReward" height = "300" style="float:left; width:30%;"></canvas>
<canvas id="canvasNonStochReward" height = "300" style="float:left; width:30%;"></canvas>

<div class="slidecontainer">
	<p>Mean/limiting loss: <span id="value">0.5</span></p>
	<input type="range" min="0" max="1" value="0.5" step = "0.05" id="myRange">
</div>

<script>
	const ctxStochReward = document.getElementById('canvasStochReward').getContext('2d');
	const ctxNonStochReward = document.getElementById('canvasNonStochReward').getContext('2d');
	var xmin = 1
	var xmax = 50
	var step = 1
	var limLoss = 0.5
	xs = []
	for (var i = xmin; i <= xmax; i++) {
		xs.push(i)
	}
	
	chartStochReward = createChart(ctxStochReward, limLoss, stochastic = true);
	chartNonStochReward = createChart(ctxNonStochReward, limLoss, stochastic = false);
	
	var slider = document.getElementById("myRange");
	var output = document.getElementById("value");
	slider.oninput = function() {
		output.innerHTML = this.value;
		limLoss = parseFloat(document.getElementById("myRange").value);
		
		var newMin, newMax;
		if (limLoss > 0.5) {
			newMin = (2 * limLoss) - 1, newMax = 1;
		}
		else {
			newMin = 0, newMax = 2 * limLoss;		
		}
		chartStochReward.data.datasets[0].function = function(x, limLoss = 0.5) { return randomInRange(newMin, newMax) };
		
		newStochData = generateData(chartStochReward, limLoss);
		chartStochReward.config.data = newStochData;
		chartStochReward.update();

		newNonStochData = generateData(chartNonStochReward, limLoss);
		chartNonStochReward.config.data = newNonStochData;
		chartNonStochReward.update();
	}
</script>

Do you see the resemblance between hyperparameter optimisation and the non-stochastic setting? Each hyperparameter configuration is going to yield a decreasing loss sequence and our goal is to identify the configuration with the lowest limiting loss.

<script src='../../assets/scripts/hyperparam.js'></script>

<div id="container">
<canvas id="canvasHyperparam" height = "400" style="width:50%;"></canvas>
<button id="buttonAdd" type="button">Add<br>hyperparameter</button>
<button id="buttonRemove" type="button">Remove<br>hyperparameter</button>
</div>

<script>
	const ctxHyperparam = document.getElementById('canvasHyperparam').getContext('2d');
	chartHyperparam = createHyperChart(ctxHyperparam);
	
	const buttonAdd = document.getElementById("buttonAdd");
	buttonAdd.addEventListener('click', function () {
		addHyperData(chartHyperparam, Math.random());
	}, false);
	
	const buttonRemove = document.getElementById("buttonRemove");
	buttonRemove.addEventListener('click', function () {
		removeHyperData(chartHyperparam);
	}, false);
</script>

## Successive Halving algorithm

Now we'll be talking about the main algorithm, Successive Halving. As mentioned in the paper, Successive Halving algorithm was first introduced in [[5]](#5) for the stochastic setting.

### Insight

The authors take advantage of the following key insight: promising configurations tend to perform great even early on in the process. Therefore there is no need to keep running the bad-performing configurations; they probably suck anyway{% sidenote 'sn-id-2' "Fine print: this is merely a heuristic for prunning the bad-performing configurations. It's not a silver bullet! Check out the sufficiency result."%}.

### Basic outline

Here we see a very basic, barebones version of the algorithm without any distracting equation or variable. By taking advantage of the previous insight, the algorithm gradually prunes{% sidenote 'sn-id-2' "This pruning action is also known as *early-stopping*."%} some of the bad-performing arms. 

{% maincolumn 'assets/img/succ_halving_informal.png' 'Algorithm 1: An informal description of the successive halving algorithm.' %}

### Demonstration

Before we go on to seeing the description of Successive Halving in full generality, let's take a look at a small sample run. Notice how the coloured regions keep getting wider over time. This is one of the key features of Successive Halving which allows the apparently promising-looking configurations run longer.

{% maincolumn 'assets/img/succ_halving_demo.png' 'Figure 2: A demo run of the Successive Halving algorithm. Here the budget ($$ B $$) is 100 and the number of arms ($$ n $$) is 8. Figure source: [[4]](#4).' %}

### Formal description

Behold the Successive Halving algorithm in all its glory!

{% maincolumn 'assets/img/succ_halving_formal.png' 'Algorithm 2: Successive Halving algorithm with all the bells and whistles.' %}

Let's take note of some of the small details that has been added.
- The number of arms ($$ n $$) needs to be specified explicitly (therefore needs to be finite){% sidenote 'sn-id-2' "This is a very big limitation. What if the hyperparameter space $$ \mathcal{H} $$ is continuous (which is often the case)?"%}. 
- After each round, as the name suggests, we prune half of the bad-performing arms. The algorithm thus runs for $$ \log_{2}(n) $$ many rounds ($$ \lceil \log_{2}(n) \rceil $$ if $$ n $$ is not a power of two).
- The number of pulls in the $$ k $$-th round $$ r_{k} $$ with $$ k $$ (precisely, it doubles as $$ k $$ is incremented by one). It is due to the fact that both $$ B $$ and $$ \log_{2}(n) $$ remains constant throughout the running of the algorithm and $$ S_{k} $$ is halved after each round.

## Theoretical guarantees

The paper includes some nice theoretical guarantees for both Successive Halving and uniform allocation. Remind, uniform allocation pulls each arm $$ \frac{B}{n} $$ times and returns the best-performing arm.

### Successive Halving

{% marginnote 'th-id-1' 'Theorem 1: Sufficiency condition of Successive Halving. If your total budget is more than $$ z $$, you can sleep tight.' %}

<div class="table-wrapper">
<table class="booktabs" border="1px solid black;">
	<tbody>
		<tr>
			<td>
				Let 
				<ul>
					<li>\( \mu_{i} = \lim_{t \to \infty} \ell_{i, t} \text{, } \)</li>
					<li>\( \overline{\gamma}(t) = \max_{i = 1, \dots, n} \gamma_{i}(t) \text{, and} \)</li>
					<li>\( z = 2 \lceil \log_{2}(n) \rceil \max_{i = 2, \dots, n} i \left( 1 + \overline{\gamma}^{-1} \left( \frac{\mu_{i} - \mu_{1}}{2} \right) \right) \text{.} \)</li>
				</ul> 
				If the budget \( B > z \) then the best arm is returned from the algorithm.
			</td>
		</tr>
	</tbody>
</table>
</div>

*Proof idea*: To be added.

### Uniform Allocation

The paper includes both the sufficient and neccessary condition of the optimality of the uniform allocation.

{% marginnote 'th-id-2' 'Theorem 2: Sufficiency condition of uniform allocation.' %}

<div class="table-wrapper">
<table class="booktabs" border="1px solid black;">
	<tbody>
		<tr>
			<td>
				Let 
				<ul>
					<li>\( \mu_{i} = \lim_{t \to \infty} \ell_{i, t} \text{, } \)</li>
					<li>\( \overline{\gamma}(t) = \max_{i = 1, \dots, n} \gamma_{i}(t) \text{, and} \)</li>
					<li>\( z = \max_{i = 2, \dots, n} n \overline{\gamma}^{-1} \left( \frac{\mu_{i} - \mu_{1}}{2} \right) \text{.} \)</li>
				</ul> 
				If the budget \( B > z \) then the best arm is returned from the algorithm.
			</td>
		</tr>
	</tbody>
</table>
</div>

*Proof idea*: To be added.

{% marginnote 'th-id-3' "Theorem 3: Necessity condition of uniform allocation. It's in fact the contrapositive of the necessity condition (i.e. $$ B < z \implies $$ Optimal arm not guaranteed to be identified)." %}

<div class="table-wrapper">
<table class="booktabs" border="1px solid black;">
	<tbody>
		<tr>
			<td>
				For any given budget \( B \) and final values \( \mu_{1} \lt \mu_{2} \leq \dots \leq \mu_{n} \) there exists a sequence of losses \( \{ \ell_{i, t} \}_{t = 1}^{\infty} \), \( i = 1, \dots, n \) such that if
				\[ B \lt \max_{i = 2, \dots, n} n \overline{\gamma}^{-1} \left( \frac{\mu_{i} - \mu_{1}}{2} \right) \]
				then the uniform budget allocation strategy will not return the best arm.
			</td>
		</tr>
	</tbody>
</table>
</div>

*Proof idea*: To be added.

Notice that it's both *necessary* and *sufficient* to have a budget ($$ B $$) of $$ \max_{i = 2, \dots, n} n \overline{\gamma}^{-1} \left( \frac{\mu_{i} - \mu_{1}}{2} \right) $$ to ensure the identification of the best arm.

## Empirical Results

The paper includes many empirical results; almost all of them shows a clear dominance of Successive Halving over the rest. Following figure demonstrate the superiority of Successive Halving over uniform allocation and Successive Rejects, another algorithm of similar flavour.

{% maincolumn 'assets/img/succ_halving_proof.png' "Figure 2: Successive Halving's performance for choosing SVM hyperparameters for four different datasets." %}

The efficiency of Successive Halving is astounding: for Australian dataset, the test error achieved by Successive Halving at 12 seconds takes 70+ seconds for Successive Rejects to reach; uniform allocation doesn't even come close. That's a $$ 6 \times $$ speedup!

## What's next?
## References

<a id="1">[1]</a> 
Shipra Agrawal and Navin Goyal.  Analysis of thompson sampling for themulti-armed bandit problem. In Conference on learning theory, pages 39–1.JMLR Workshop and Conference Proceedings, 2012.

<a id="2">[2]</a> 
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thomp-son sampling. In Artificial intelligence and statistics, pages 99–107. PMLR, 2013.

<a id="3">[3]</a> 
S ́ebastien Bubeck, R ́emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits problems. In International conference on Algorithmic learning theory, pages 23–37. Springer, 2009.

<a id="4">[4]</a> 
Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, editors. Automated Machine Learning: Methods, Systems, Challenges. Springer, 2018. In press, available at http://automl.org/book.

<a id="5">[5]</a> 
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal explorationin multi-armed bandits. In International Conference on Machine Learning, pages 1238–1246. PMLR, 2013.

<a id="6">[6]</a> 
Emilie Kaufmann, Nathaniel Korda, and R ́emi Munos. Thompson sampling: An asymptotically optimal finite-time analysis.  In International conference on algorithmic learning theory, pages 199–213. Springer, 2012.














