---
layout: post
title:  "Non-stochastic Best Arm Identification and Hyperparameter Optimization"
date:   2021-08-22
categories: post
---

How can you find the optimal value of the hyperparameter of your model? This paper addresses this problem and introduces a novel framework for modelling this task. The authors use the well-known Successive Halving algorithm for producing optimal/near-optimal solution. The paper include some nice theoretical guarantees as well as empirical results in favour of the algorithm.
<!--more-->

{% marginnote 'table-id-1' 'Table 1: Paper summary.' %}

<div class="table-wrapper">
<table class="booktabs" style="margin: 0px auto;">
	<tbody>
		<tr>
			<td>Author(s)</td> <td>Kevin Jamieson, Ameet Talwalkar</td>
		</tr>
		<tr>
			<td>Source</td> <td><a href="https://arxiv.org/abs/1502.07943">https://arxiv.org/abs/1502.07943</a></td>
		</tr>
		<tr>
			<td>Danger level</td> <td>☠️ ☠️ <span style="color:gray">☠ ☠ ☠</span></td>
		</tr>
	</tbody>
</table>
</div>

## Contributions
- Introduction of the non-stochastic setting in the best arm identification problem.
- Analysis of the Successive Halving algorithm in the non-stochastic setting.
- Modelling of the hyperparameter optimisation as a non-stochastic best arm identification problem.
- Empirical proof of efficiency of Successive Halving in hyperparameter optimisation.

## Model parameter and model hyperparameter

Consider this scenario: given two models, $$ \mathcal{M}_{1} $$ and $$ \mathcal{M}_{2} $$, what would differentiate them? What makes a model different from another? For the moment, assume that we are dealing with the same kind of models (say, $$ \mathcal{M}_{1} $$ and $$ \mathcal{M}_{2} $$ are both neural networks or both decision trees but not one neural network and the other decision tree).

The answer is their parameters and hyperparameters. For ANN, the parameters are the set of weights; the hyperparameters are the structure of the network, the number of neurons in each layer, the choice of activation function etc. For decision tree, the parameters are the set of split points; the hyperparameters are the criterion, maximum depth, maximum number of features etc.

### Parameter vs. hyperparameter

That said, there is one crucial difference between parameters and hyperparameters. Given the data, the optimal value of the parameters can be *learned* from the data. Hyperparameters, on the other hand, can't be learned *directly*.

<script src='https://cdn.jsdelivr.net/npm/chart.js'></script>
<script src='../../assets/scripts/utils.js'></script>
<script src='../../assets/scripts/reward.js'></script>
<script src='../../assets/scripts/serial.js'></script>
<script src='../../assets/scripts/search.js'></script>
<script src='../../assets/scripts/parallel.js'></script>

<div id="canvasContainer1">
	<canvas id="canvasSearchRandom" height = "300" style="float:left; width:30%;"></canvas>
	<canvas id="canvasSearchGrid"   height = "300" style="float:left; width:30%;"></canvas>
</div>

{% marginnote 'tool-id-1' "Searching for the optimal hyperparameter configuration across different combinations of values for two hyperparameters: $$ x_{1} $$ and $$ x_{2} $$. Random search and grid search are two popular search algorithms. Check out the Wikipedia page of [hyperparameter optimisation](https://en.wikipedia.org/wiki/Hyperparameter_optimization), it's pretty good." %}

<div id="buttonContainer1" height = "300" style="width:60%; display:flex; align-items:center;">
	<button id="buttonSearchNext"  type="button" style="height:40px; width:120px; margin-left:auto; margin-right:10px;">Next<br>configuration</button>
	<button id="buttonSearchClear" type="button" style="height:40px; width:120px; margin-left:10px; margin-right:auto;">Clear</button>
</div>

<script>
	const ctxRandom = document.getElementById('canvasSearchRandom').getContext('2d');
	const ctxGrid   = document.getElementById('canvasSearchGrid').getContext('2d');
	
	var chartRandom = createSearchChart(ctxRandom, false);
	var chartGrid   = createSearchChart(ctxGrid, true);
	
	const numSamples = 15;
	const xmin = 1, 
		  xmax = 50,
		  step = 1;
	var limLoss = 0.5;
	var xs = [];
	for (var i = xmin; i <= xmax; i++) {
		xs.push(i);
	}
	
	const buttonSearchNext = document.getElementById("buttonSearchNext");
	buttonSearchNext.addEventListener('click', function () {
		selectNextHyperparameterRandom(chartRandom);
		selectNextHyperparameterGrid(chartGrid);
	}, false);
	
	const buttonSearchClear = document.getElementById("buttonSearchClear");
	buttonSearchClear.addEventListener('click', function () {
		clearChart(chartRandom);
		clearChart(chartGrid);
	}, false);
</script>

### Serial vs. parallel

Say you want to know the best [activation function](https://en.wikipedia.org/wiki/Activation_function) for your model. Again, say there are four alternatives in your hand: sigmoid, tanh, ReLU, and ELU. The exact functions are not super-important here, instead the number of available choices are. As any mathematician would say, we better put those four *things* in a set and give it a nice name.

Formally, we say that each hyperparameter has an associated hyperparameter space $$ \mathcal{H} $$, where $$ \mathcal{H} = \{ h_{1}, h_{2}, \dots, h_{n} \} $${% sidenote 'sn-id-1' "Each hyperparameter choice $$ h \in \mathcal{H} $$ is often called a hyperparameter configuration."%} (for the entirety of this post we assume that $$ \mathcal{H} $$ is finite). For learning rate, for example, the hyperparameter space is $$ \mathbb{R}_{>0} $$ (the set of positive numbers). There are at least two different ways in which you can handle the task of finding out the best hyperparameter configuration: *serial* and *parallel*.

Under the *serial* setting we have the following workflow. Choose a suitable hyperparameter configuration from $$ \mathcal{H} $$, run it for an *appropriate* amount of time to get a good estimate of the loss, and then depending on this loss choose a new hyperparameter configuration from $$ \mathcal{H} $$. Continue this as long as time permits.

{% marginnote 'tool-id-3' "In the *serial* mode, we consider one configuration at a time. Once we are done with it, go for the next one." %}

<div id="canvasContainer2">
	<canvas id="canvasSerial" height = "400" style="width:50%;"></canvas>
</div>

<div id="buttonContainer3" height="300" style="width:50%; display:flex; align-items:center;">
	<button id="buttonSerialNext"    type="button" style="height:40px; width:120px; margin-left:auto; margin-right:10px;">Next<br>configuration</button>
	<button id="buttonSerialClear" type="button" style="height:40px; width:120px; margin-left:10px; margin-right:auto;">Clear</button>
</div>

<script>
	const ctxSerial = document.getElementById('canvasSerial').getContext('2d');
	chartSerial = createSerialChart(ctxSerial);
	const numConfigurations = 4;
	
	const buttonSerialNext = document.getElementById("buttonSerialNext");
	buttonSerialNext.addEventListener('click', function () {
		addHyperData(chartSerial, Math.random());
	}, false);
	
	const buttonSerialClear = document.getElementById("buttonSerialClear");
	buttonSerialClear.addEventListener('click', function () {
		clearSerialChart(chartSerial);
	}, false);
</script>

The *parallel* setting is slightly different. Here we start with *all* the hyperparameter configurations i.e. all $$ h \in \mathcal{H} $$. We allot a certain amount of time for the configurations, get an estimate of the *partial* loss after the allotted time, take some actions if needed, and then allot more time to the configurations. Continue this as long as time permits.

{% marginnote 'tool-id-4' "In the *parallel* mode, we consider *all* the configurations at the same time. By the way, we do not necessarily need to *process* them at the same time (it would put a lot of burden on the hardware if the number of configurations is huge); we can do so serially as well. The point is that we *allot* the budget at the same time." %}

<canvas id="canvasParallel" height = "400" style="width:50%;"></canvas>

<div id="buttonContainer2" height="300" style="width:50%; display:flex; align-items:center;">
	<button id="buttonParallelAllot" type="button" style="height:40px; width:120px; margin-left:auto; margin-right:10px;">Allot<br>more budget</button>
	<button id="buttonParallelClear" type="button" style="height:40px; width:120px; margin-left:10px; margin-right:auto;">Clear</button>
</div>

<script>
	const ctxParallel = document.getElementById('canvasParallel').getContext('2d');
	chartParallel = createParallelChart(ctxParallel);
	const numPhases = 4;
	
	const buttonParallelAllot = document.getElementById("buttonParallelAllot");
	buttonParallelAllot.addEventListener('click', function () {
		allotBudget(chartParallel);
	}, false);
	
	const buttonParallelClear = document.getElementById("buttonParallelClear");
	buttonParallelClear.addEventListener('click', function () {
		clearParallelChart(chartParallel);
	}, false);
</script>

The difference is pretty obvious if we take a look at the plots--*serial* evaluation progresses *vertically* whereas the *parallel* evaluation progresses *horizontally*. This post is entirely dedicated to this *parallel* setting.

### Hyperparameter optimisation

Hyperparameter optimisation is therefore concerned about finding out the optimal value of the hyperparameter for which the model loss is minimum. Obviously, we have the brute-force approach: starting with a total budget of $$ B $$ units (minutes, hours, clock-cycles etc.) and $$ n $$ hyperparameter choices, allocate $$ \frac{B}{n} $$ units of resource to all the choices. Finally return the hyperparameter with the lowest loss. This is often called the uniform allocation strategy.

But can we do better? Before we answer this question, let's talk a little bit about slot machines and gambling.

## Multi-armed bandit and best arm identification

The problem setting of [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) (MAB) is extremely simple: a gambler enters a room with multiple slot machines and (s)he has a fixed, limited amount of money/chances. (S)he has to play the slot machines in such a way that her/his gain is maximised.

The only problem is that (s)he has no idea which machines are good and which are bad. Good ones yield rewards more easily compared to the bad ones. Thus (s)he has to use trial-and-error to gain knowledge about the machines. Contrary to the simplicity of the problem, good/great solutions are not that straightforward.

MAB has a long history, arguably starting in 1933 with the [classic paper of Thompson](https://www.jstor.org/stable/2332286){% sidenote 'sn-id-2' "Quite strangely, Thompson sampling, as it's known these days, turned out to be quite hard to analyse theoretically (fun fact: the proof that Thompson sampling is asymptotically optimal came out quite recently (2012-13) in a series of papers ([[1]](#1), [[2]](#2), and [[6]](#6)), so that's almost 80 years)."%}. There are many good, quick introductory resources available online--[this video](https://youtu.be/bkw6hWvh_3k) by Robert C. Gray (aka Academic Gamer) is a great starting point. If you want something more in-depth, take a look at [this excelent post](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html) by Lilian Weng.

Let us now shift our attention towards a slightly different, yet related problem--the best-arm identification problem{% sidenote 'sn-id-3' "It's also known as the pure exploration problem in the bandit literature."%}. Here we simply want to identify the *best* arm{% sidenote 'sn-id-4' "We use the terms *arm* and *slot machine* interchangeably."%}. The precise definition of *best*, as we're going to see next, is dependent on the problem setting. 

So after $$ T $$ rounds, our agent is going to be judged based on the quality of the slot machine that it recommends. In practice, though, we deal with a slightly more generalised problem: instead of a single recommendation after $$ T $$ rounds, we want the agent to recommend a slot machine in each round. Check out [[3]](#3) for more details.

## Stochastic vs. non-stochastic setting

Let's now discuss how we define the *best* arm. Since we'll be talking about model *losses* later, let's be consistent and say that each time we pull the lever of a slot machine we observe a *loss* instead of a *reward*. Thus we want to find out the slot with the lowest loss. Say $$ \ell_{i, t} $$ denotes the loss observed on the $$ t $$-th pull of the $$ i $$-th slot machine.

In the stochastic setting each time one pulls a slot machine a loss is sampled from a fixed distribution with mean $$ \mu $${% sidenote 'sn-id-5' "We'll use $$ \mu $$ to denote the mean of the distribution in the stochastic setting. For the non-stochastic setting, we'll go with the authors and use $$ \nu $$ to denote the limiting loss."%} i.e. 

$$ \mathbb{E}[\ell_{i, t}] = \mu_{i} \text{.} $$

In other words, if we take a large number of such losses from the $$ k $$-th slot machine (or rewards, if that makes more sense) and compute the average, it would be exactly or very close to $$ \mu_{k} $$.

The non-stochastic setting, on the other hand, says that there exists a finite *limiting loss* for each slot machine; each time one pulls the slot machine the generated loss would be a little bit closer to that *limiting loss*. In mathematical terms it is written as

$$ \lim_{t \to \infty} \ell_{i, t} = \nu_{k} \text{.} $$

{% marginnote 'tool-id-2' "Stochastic versus the non-stochastic setting." %}

<canvas id="canvasStochReward"    height = "300" style="float:left; width:28%;"></canvas>
<canvas id="canvasNonStochReward" height = "300" style="float:left; width:28%;"></canvas>

<div id="sliderContainer" height="300" style="width:60%; align-items:center;">
	<p style="display:block;">Mean/limiting loss: <span id="value">0.5</span></p>
	<input type="range" min="0" max="1" value="0.5" step="0.05" id="myRange" style="display:block; padding:20px 0px 20px 0px; margin:auto;">
</div>

<script>
	const ctxStochReward    = document.getElementById('canvasStochReward').getContext('2d');
	const ctxNonStochReward = document.getElementById('canvasNonStochReward').getContext('2d');
	
	var chartStochReward    = createChart(ctxStochReward, limLoss, stochastic = true);
	var chartNonStochReward = createChart(ctxNonStochReward, limLoss, stochastic = false);
	
	var slider = document.getElementById("myRange");
	var output = document.getElementById("value");
	slider.oninput = function() {
		output.innerHTML = this.value;
		limLoss = parseFloat(document.getElementById("myRange").value);
		
		var newMin, newMax;
		if (limLoss > 0.5) {
			newMin = (2 * limLoss) - 1, newMax = 1;
		}
		else {
			newMin = 0, newMax = 2 * limLoss;		
		}
		chartStochReward.data.datasets[0].function = function(x, limLoss = 0.5) { return randomInRange(newMin, newMax) };
		
		newStochData = generateData(chartStochReward, limLoss);
		chartStochReward.config.data = newStochData;
		chartStochReward.update();

		newNonStochData = generateData(chartNonStochReward, limLoss);
		chartNonStochReward.config.data = newNonStochData;
		chartNonStochReward.update();
	}
</script>

Do you see the resemblance between hyperparameter optimisation and the non-stochastic setting? Each hyperparameter configuration is going to yield a decreasing loss sequence and our goal is to identify the configuration with the lowest limiting loss.

## Successive Halving algorithm

Now we'll be talking about the main algorithm, Successive Halving. As mentioned in the paper, Successive Halving algorithm was first introduced in [[5]](#5) for the stochastic setting.

### Insight

The authors take advantage of the following key insight: promising configurations tend to perform great even early on in the process. Therefore there is no need to keep running the bad-performing configurations; they probably suck anyway{% sidenote 'sn-id-6' "Fine print: this is merely a heuristic for pruning the bad-performing configurations. It's not a silver bullet! Check out the sufficiency result."%}.

### Basic outline

Here we see a very basic, barebones version of the algorithm without any distracting equation or variable. By taking advantage of the previous insight, the algorithm gradually prunes (also called *early-stopping*) some of the bad-performing arms. 

{% maincolumn 'assets/img/succ_halving_informal.png' 'Algorithm 1: An informal description of the successive halving algorithm.' %}

### Demonstration

Before we go on to seeing the description of Successive Halving in full generality, let's take a look at a small sample run. Notice how the coloured regions keep getting wider over time. This is one of the key features of Successive Halving which allows the apparently promising-looking configurations run longer.

{% maincolumn 'assets/img/succ_halving_demo.png' 'Figure 1: A demo run of the Successive Halving algorithm. Here the budget ($$ B $$) is 100 and the number of arms ($$ n $$) is 8. Figure source: [[4]](#4).' %}

### Formal description

Behold the Successive Halving algorithm in all its glory!

{% maincolumn 'assets/img/succ_halving_formal.png' 'Algorithm 2: Successive Halving algorithm with all the bells and whistles.' %}

Let's take note of some of the small details that has been added.
- The number of arms ($$ n $$) needs to be specified explicitly (therefore needs to be finite){% sidenote 'sn-id-7' "This is a very big limitation. What if the hyperparameter space $$ \mathcal{H} $$ is continuous (which is often the case)?"%}. 
- After each round, as the name suggests, we prune half of the bad-performing arms. The algorithm thus runs for $$ \log_{2}(n) $$ many rounds ($$ \lceil \log_{2}(n) \rceil $$ if $$ n $$ is not a power of two).
- The number of pulls in the $$ k $$-th round $$ r_{k} $$ with $$ k $$ (precisely, it doubles as $$ k $$ is incremented by one). It is due to the fact that both $$ B $$ and $$ \log_{2}(n) $$ remains constant throughout the running of the algorithm and $$ S_{k} $$ is halved after each round.

## Theoretical guarantees

This is going to be quite dense and math-heavy. The paper includes some nice theoretical guarantees for both Successive Halving and uniform allocation. Remind, uniform allocation pulls each arm $$ \frac{B}{n} $$ times and returns the best-performing arm.

### Successive Halving

{% marginnote 'th-id-1' 'Theorem 1: Sufficiency condition of Successive Halving. If your total budget is more than $$ z $$, you can sleep tight.' %}

<div class="table-wrapper">
<table class="booktabs" border="1px solid black;">
	<tbody>
		<tr>
			<td>
				Let 
				<ul>
					<li>\( \nu_{i} = \lim_{t \to \infty} \ell_{i, t} \text{, } \)</li>
					<li>\( \overline{\gamma}(t) = \max_{i = 1, \dots, n} \gamma_{i}(t) \text{, and} \)</li>
					<li>\( z = 2 \lceil \log_{2}(n) \rceil \max_{i = 2, \dots, n} i \left( 1 + \overline{\gamma}^{-1} \left( \frac{\nu_{i} - \nu_{1}}{2} \right) \right) \text{.} \)</li>
				</ul> 
				If the budget \( B > z \) then the best arm is returned from the algorithm.
			</td>
		</tr>
	</tbody>
</table>
</div>

*Proof idea*: To be added.

### Uniform Allocation

The paper includes both the sufficient and necessary condition of the optimality of the uniform allocation.

{% marginnote 'th-id-2' 'Theorem 2: Sufficiency condition of uniform allocation.' %}

<div class="table-wrapper">
<table class="booktabs" border="1px solid black;">
	<tbody>
		<tr>
			<td>
				Let 
				<ul>
					<li>\( \nu_{i} = \lim_{t \to \infty} \ell_{i, t} \text{, } \)</li>
					<li>\( \overline{\gamma}(t) = \max_{i = 1, \dots, n} \gamma_{i}(t) \text{, and} \)</li>
					<li>\( z = \max_{i = 2, \dots, n} n \overline{\gamma}^{-1} \left( \frac{\nu_{i} - \nu_{1}}{2} \right) \text{.} \)</li>
				</ul> 
				If the budget \( B > z \) then the best arm is returned from the algorithm.
			</td>
		</tr>
	</tbody>
</table>
</div>

*Proof idea*: To be added.

{% marginnote 'th-id-3' "Theorem 3: Necessity condition of uniform allocation. It's in fact the contrapositive of the necessity condition (i.e. $$ B < z \implies $$ Optimal arm not guaranteed to be identified)." %}

<div class="table-wrapper">
<table class="booktabs" border="1px solid black;">
	<tbody>
		<tr>
			<td>
				For any given budget \( B \) and final values \( \nu_{1} \lt \nu_{2} \leq \dots \leq \nu_{n} \) there exists a sequence of losses \( \{ \ell_{i, t} \}_{t = 1}^{\infty} \), \( i = 1, \dots, n \) such that if
				\[ B \lt \max_{i = 2, \dots, n} n \overline{\gamma}^{-1} \left( \frac{\nu_{i} - \nu_{1}}{2} \right) \]
				then the uniform budget allocation strategy will not return the best arm.
			</td>
		</tr>
	</tbody>
</table>
</div>

*Proof idea*: To be added.

Notice that it's both *necessary* and *sufficient* to have a budget ($$ B $$) of $$ \max_{i = 2, \dots, n} n \overline{\gamma}^{-1} \left( \frac{\nu_{i} - \nu_{1}}{2} \right) $$ to ensure the identification of the best arm.

## Empirical Results

The paper includes many empirical results; almost all of them shows a clear dominance of Successive Halving over the rest. Following figure demonstrate the superiority of Successive Halving over uniform allocation and Successive Rejects, another algorithm of similar flavour.

{% maincolumn 'assets/img/succ_halving_proof.png' "Figure 2: Successive Halving's performance for choosing SVM hyperparameters for four different datasets." %}

The efficiency of Successive Halving is astounding: in the top-left figure, the test error achieved by Successive Halving at 12 seconds takes 70+ seconds for Successive Rejects to reach; uniform allocation doesn't even come close. That's a $$ 6 \times $$ speedup!

## What's next?
## References

<a id="1">[1]</a> 
Shipra Agrawal and Navin Goyal.  Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pages 39–1.JMLR Workshop and Conference Proceedings, 2012.

<a id="2">[2]</a> 
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Artificial intelligence and statistics, pages 99–107. PMLR, 2013.

<a id="3">[3]</a> 
S ́ebastien Bubeck, R ́emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits problems. In International conference on Algorithmic learning theory, pages 23–37. Springer, 2009.

<a id="4">[4]</a> 
Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, editors. Automated Machine Learning: Methods, Systems, Challenges. Springer, 2018. In press, available at http://automl.org/book.

<a id="5">[5]</a> 
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In International Conference on Machine Learning, pages 1238–1246. PMLR, 2013.

<a id="6">[6]</a> 
Emilie Kaufmann, Nathaniel Korda, and R ́emi Munos. Thompson sampling: An asymptotically optimal finite-time analysis.  In International conference on algorithmic learning theory, pages 199–213. Springer, 2012.

















